{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests de la Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mecanismo de Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase de autoatención implementada por nuestro modelo es la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, inpunt_dim, hidden_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.inpunt_dim = inpunt_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.Q = nn.Linear(inpunt_dim, hidden_dim, bias=False)\n",
    "        self.K = nn.Linear(inpunt_dim, hidden_dim, bias=False)\n",
    "        self.V = nn.Linear(inpunt_dim, hidden_dim, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xq = self.Q(x)\n",
    "        xk = self.K(x)\n",
    "        xv = self.V(x)\n",
    "\n",
    "        scores = torch.bmm(xq, xk.transpose(1, 2))\n",
    "        scores = scores / (self.hidden_dim ** 0.5)\n",
    "        mask = self._mask(scores).to(x.device)\n",
    "        scores = scores + mask\n",
    "\n",
    "        scores = self.softmax(scores)\n",
    "        attention = torch.bmm(scores, xv)\n",
    "        return scores, attention\n",
    "\n",
    "    def _mask(self, x):\n",
    "        mask = torch.tril(torch.ones(x.size(1), x.size(1)), diagonal=0)\n",
    "        mask[mask == 0] = float('-inf')\n",
    "        mask[mask == 1] = 0\n",
    "        mask = mask.repeat(x.size(0), 1, 1)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora bien, esta clase está preparada para recibir un tensor de entrada, y calcular sobre ellos la atención. No obstante, se pide en la práctica comprobar que el mecanismo de atención funcione correctamente sobre las matrices Q, K y V, directamente. Para ello, se aplican unas pequeñas modificaciones a esta clase, que permiten realizar el cálculo de la atención sobre dichas matrices, sin necesidad de un tensor de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.Q = nn.Linear(input_dim, hidden_dim, bias=False)\n",
    "        self.K = nn.Linear(input_dim, hidden_dim, bias=False)\n",
    "        self.V = nn.Linear(input_dim, hidden_dim, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, Q=None, K=None, V=None, x=None):\n",
    "        # Se comprueba si se recibe un tensor x de entrada\n",
    "        if x is not None:\n",
    "            Q = self.Q(x)\n",
    "            K = self.K(x)\n",
    "            V = self.V(x)\n",
    "\n",
    "        # Cuando no se recibe un tensor x de entrada, hay que añadir una dimensión para el batch\n",
    "        if Q.dim() == 2:\n",
    "            Q = Q.unsqueeze(0)\n",
    "            K = K.unsqueeze(0)\n",
    "            V = V.unsqueeze(0)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(1, 2))\n",
    "        scores = scores / (self.hidden_dim ** 0.5)\n",
    "        mask = self._mask(scores).to(Q.device)\n",
    "        scores = scores + mask\n",
    "\n",
    "        scores = self.softmax(scores)\n",
    "        attention = torch.matmul(scores, V)\n",
    "        return scores.squeeze(0), attention.squeeze(0)\n",
    "\n",
    "    def _mask(self, x):\n",
    "        mask = torch.tril(torch.ones(x.size(1), x.size(1)), diagonal=0)\n",
    "        mask[mask == 0] = float('-inf')\n",
    "        mask[mask == 1] = 0\n",
    "        mask = mask.repeat(x.size(0), 1, 1)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testearemos ahora la atención sobre las siguientes matrices Q, K y V, dado que ya conocemos de antemano el resultado numérico que debería obtenerse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.tensor([[0.0, 0.0, 0.0], [1, 1, 1], [0.2, 0.2, 0.2], [0.3, 0.3, 0.3]])\n",
    "K = torch.tensor([[0.1, 0.1, 0.1], [0.2, 0.2, 0.2], [0.3, 0.3, 0.3], [0.4, 0.4, 0.4]])\n",
    "V = torch.tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 1., 1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "att = SelfAttention(3, 3)\n",
    "scores, attention = att(Q=Q, K=K, V=V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4568, 0.5432, 0.0000, 0.0000],\n",
      "        [0.3219, 0.3332, 0.3449, 0.0000],\n",
      "        [0.2309, 0.2432, 0.2561, 0.2698]])\n",
      "\n",
      "Attention:\n",
      " tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.4568, 0.5432, 0.0000],\n",
      "        [0.3219, 0.3332, 0.3449],\n",
      "        [0.2309, 0.5130, 0.5260]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Scores:\\n\", scores)\n",
    "print(\"\\nAttention:\\n\", attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En efecto, se obtiene el resultado deseado, por lo que la atención se ha calculado con éxito."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
